{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchbnn as bnn\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import pickle as pkl\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake dimensions (lat, lon, months, time)\n",
    "n_years = 10\n",
    "lats = np.arange(30, 50, 0.5)\n",
    "lons = np.arange(-10,20, 0.5)\n",
    "# Change time so the in sample runs -1, 1\n",
    "time = np.arange(-1,1, (2/12)/n_years) * 2 + 1\n",
    "months = np.tile(np.arange(-1,1,2/12), n_years)\n",
    "\n",
    "# Create smooth function to serve as the observational truth\n",
    "def fun(time, lat, lon, month):\n",
    "    f = (0.5 * (((lat / 90) ** 2) + 0.5 * np.sin(2 * np.pi * lon / 180)) - 0.2 * np.cos(np.pi * month) + \n",
    "         0.1 * np.sin(4 * np.pi * lat / 90) * np.cos(4 * np.pi * lon / 180) - 0.05 * np.cos(2 * np.pi * time) + 0.05 * np.sin(2 * np.pi * lat / 45) * np.sin(2 * np.pi * lon / 90)) \n",
    "    return f\n",
    "\n",
    "obs = np.zeros([len(time), len(lats), len(lons)])\n",
    "for i, t in enumerate(time):\n",
    "    for j, lat in enumerate(lats):\n",
    "        for k, lon in enumerate(lons):\n",
    "            obs[i, j, k] = fun(t, lat, lon, months[i])\n",
    "            \n",
    "# Normalise obs\n",
    "obs = 2 * (obs - obs.min())/(obs.max() - obs.min()) - 1\n",
    "\n",
    "\n",
    "# Format data so it is suitable for NN input\n",
    "data_len = obs.size\n",
    "lon_data = np.tile(lons, int(data_len / len(lons))).reshape(-1,)\n",
    "lat_data = np.tile(np.repeat(lats, len(lons)), len(time)).reshape(-1,)\n",
    "time_data = np.repeat(time, int(data_len / len(time))).reshape(-1,)\n",
    "mon_data = np.repeat(months, int(data_len / len(time))).reshape(-1,)\n",
    "\n",
    "# model 1  True in the north with a -0.03 bias\n",
    "mdl1 = obs.copy() - 0.03 + np.random.normal(size=[len(time), len(lats), len(lons)]) * 0.005\n",
    "mdl1[:,:-16, :] = np.random.random([len(time), len(lats) - 16, len(lons)]) * 2 - 1\n",
    "\n",
    "# model 2 True around equator with no bias\n",
    "mdl2 = obs.copy() + np.random.normal(size=[len(time),len(lats), len(lons)]) * 0.005 \n",
    "mdl2[:,:16, :] = np.random.random([len(time), 16, len(lons)]) * 2 - 1\n",
    "mdl2[:,-16:, :] = np.random.random([len(time), 16, len(lons)]) * 2 - 1\n",
    "\n",
    "# model 3 True around equator with no bias (as per model 2)\n",
    "mdl3 = obs.copy() + np.random.normal(size=[len(time), len(lats), len(lons)]) * 0.005\n",
    "mdl3[:,:16, :] = np.random.random([len(time), 16, len(lons)]) * 2 - 1\n",
    "mdl3[:,-16:, :] = np.random.random([len(time), 16, len(lons)]) * 2 - 1\n",
    "\n",
    "# model 4 True in the south with 0.03 bias. True only for months 1-6\n",
    "mdl4 = obs.copy() + 0.03 + np.random.normal(size=[len(time), len(lats), len(lons)]) * 0.005\n",
    "mdl4[:,16:, :] = np.random.random([len(time), len(lats) - 16, len(lons)]) * 2 - 1\n",
    "\n",
    "# Add noise\n",
    "# In the north we have 0.01 noise\n",
    "obs[:,-16:, :] = obs[:,-16:, :] + np.random.normal(size=obs[:,-16:, :].shape) * 0.01\n",
    "# Around the equator we have 0.02 noise\n",
    "obs[:,16:-16, :] = obs[:,16:-16, :] + np.random.normal(size=obs[:,16:-16, :].shape) * 0.02\n",
    "# In the south we have 0.03 noise\n",
    "obs[:,:16, :] = obs[:,:16, :] + np.random.normal(size=obs[:,:16, :].shape) * 0.03\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['mdl1'] = mdl1.ravel()\n",
    "df['mdl2'] = mdl2.ravel()\n",
    "df['mdl3'] = mdl3.ravel()\n",
    "df['mdl4'] = mdl4.ravel()\n",
    "\n",
    "# Convert the coordinates to have a 1:1 mapping\n",
    "x = np.cos(lat_data * np.pi / 180)  * np.cos(lon_data * np.pi / 180)\n",
    "y = np.cos(lat_data * np.pi / 180)  * np.sin(lon_data * np.pi / 180)\n",
    "z = np.sin(lat_data * np.pi / 180)\n",
    "\n",
    "rads = (mon_data * 360) * (np.pi / 180)\n",
    "x_mon = np.sin(rads)\n",
    "y_mon = np.cos(rads)\n",
    "\n",
    "# Coordinate scaling\n",
    "df['x'] = x * 2\n",
    "df['y'] = y * 2\n",
    "df['z'] = z * 2\n",
    "df['x_mon'] = x_mon\n",
    "df['y_mon'] = y_mon\n",
    "df['time'] = time_data\n",
    "\n",
    "df['obs'] = obs.ravel()\n",
    "\n",
    "# Remove last 10 years to see extrapolation\n",
    "df_in = df[:int(10 * len(df)/20)]\n",
    "df_out = df[int(10 * len(df)/20):]\n",
    "\n",
    "df_shuffled = df_in.sample(frac=1, random_state=seed)\n",
    "split_idx = round(len(df_shuffled) * 0.85)\n",
    "df_train = df_shuffled[:split_idx]\n",
    "df_test = df_shuffled[split_idx:]\n",
    "\n",
    "# In sample training\n",
    "X_train = df_train.drop(['obs'],axis=1).values\n",
    "y_train = df_train['obs'].values.reshape(-1,1)\n",
    "\n",
    "# The in sample testing - this is not used for training\n",
    "X_test = df_test.drop(['obs'],axis=1).values\n",
    "y_test = df_test['obs'].values.reshape(-1,1)\n",
    "\n",
    "# For out of sample extraploation\n",
    "X_out = df_out.drop(['obs'],axis=1).values\n",
    "y_out = df_out['obs'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mdl1</th>\n",
       "      <th>mdl2</th>\n",
       "      <th>mdl3</th>\n",
       "      <th>mdl4</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x_mon</th>\n",
       "      <th>y_mon</th>\n",
       "      <th>time</th>\n",
       "      <th>obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.308125</td>\n",
       "      <td>0.593262</td>\n",
       "      <td>0.855573</td>\n",
       "      <td>-0.081189</td>\n",
       "      <td>1.705737</td>\n",
       "      <td>-0.300767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.152023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141808</td>\n",
       "      <td>-0.525250</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>-0.084253</td>\n",
       "      <td>1.708297</td>\n",
       "      <td>-0.285871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.125962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003159</td>\n",
       "      <td>-0.219612</td>\n",
       "      <td>-0.537756</td>\n",
       "      <td>-0.078845</td>\n",
       "      <td>1.710726</td>\n",
       "      <td>-0.270952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.135723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.625610</td>\n",
       "      <td>0.825712</td>\n",
       "      <td>-0.083226</td>\n",
       "      <td>1.713026</td>\n",
       "      <td>-0.256013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.149991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.235543</td>\n",
       "      <td>0.764452</td>\n",
       "      <td>0.358199</td>\n",
       "      <td>-0.092031</td>\n",
       "      <td>1.715195</td>\n",
       "      <td>-0.241055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.449294e-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.118857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143995</th>\n",
       "      <td>0.702084</td>\n",
       "      <td>-0.642518</td>\n",
       "      <td>-0.155602</td>\n",
       "      <td>-0.697434</td>\n",
       "      <td>1.238779</td>\n",
       "      <td>0.390586</td>\n",
       "      <td>1.520812</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.708805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143996</th>\n",
       "      <td>0.695548</td>\n",
       "      <td>0.261793</td>\n",
       "      <td>0.124474</td>\n",
       "      <td>-0.513974</td>\n",
       "      <td>1.235324</td>\n",
       "      <td>0.401381</td>\n",
       "      <td>1.520812</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.724898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143997</th>\n",
       "      <td>0.708643</td>\n",
       "      <td>0.068291</td>\n",
       "      <td>0.408332</td>\n",
       "      <td>-0.604038</td>\n",
       "      <td>1.231774</td>\n",
       "      <td>0.412146</td>\n",
       "      <td>1.520812</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.735268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143998</th>\n",
       "      <td>0.704098</td>\n",
       "      <td>-0.078892</td>\n",
       "      <td>-0.080094</td>\n",
       "      <td>-0.873401</td>\n",
       "      <td>1.228130</td>\n",
       "      <td>0.422879</td>\n",
       "      <td>1.520812</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.740708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143999</th>\n",
       "      <td>0.714525</td>\n",
       "      <td>0.155655</td>\n",
       "      <td>0.467884</td>\n",
       "      <td>-0.264090</td>\n",
       "      <td>1.224393</td>\n",
       "      <td>0.433580</td>\n",
       "      <td>1.520812</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.757544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mdl1      mdl2      mdl3      mdl4         x         y         z  \\\n",
       "0       0.308125  0.593262  0.855573 -0.081189  1.705737 -0.300767  1.000000   \n",
       "1       0.141808 -0.525250  0.467150 -0.084253  1.708297 -0.285871  1.000000   \n",
       "2      -0.003159 -0.219612 -0.537756 -0.078845  1.710726 -0.270952  1.000000   \n",
       "3       0.009407  0.625610  0.825712 -0.083226  1.713026 -0.256013  1.000000   \n",
       "4      -0.235543  0.764452  0.358199 -0.092031  1.715195 -0.241055  1.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "143995  0.702084 -0.642518 -0.155602 -0.697434  1.238779  0.390586  1.520812   \n",
       "143996  0.695548  0.261793  0.124474 -0.513974  1.235324  0.401381  1.520812   \n",
       "143997  0.708643  0.068291  0.408332 -0.604038  1.231774  0.412146  1.520812   \n",
       "143998  0.704098 -0.078892 -0.080094 -0.873401  1.228130  0.422879  1.520812   \n",
       "143999  0.714525  0.155655  0.467884 -0.264090  1.224393  0.433580  1.520812   \n",
       "\n",
       "               x_mon  y_mon      time       obs  \n",
       "0       2.449294e-16    1.0 -1.000000 -0.152023  \n",
       "1       2.449294e-16    1.0 -1.000000 -0.125962  \n",
       "2       2.449294e-16    1.0 -1.000000 -0.135723  \n",
       "3       2.449294e-16    1.0 -1.000000 -0.149991  \n",
       "4       2.449294e-16    1.0 -1.000000 -0.118857  \n",
       "...              ...    ...       ...       ...  \n",
       "143995 -8.660254e-01    0.5  0.966667  0.708805  \n",
       "143996 -8.660254e-01    0.5  0.966667  0.724898  \n",
       "143997 -8.660254e-01    0.5  0.966667  0.735268  \n",
       "143998 -8.660254e-01    0.5  0.966667  0.740708  \n",
       "143999 -8.660254e-01    0.5  0.966667  0.757544  \n",
       "\n",
       "[144000 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([122400, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"C:/Users/Artgur/Desktop/Uni/MA/BNN/Models/ToyPyModels_BNN_v4/\"\n",
    "\n",
    "\n",
    "num_models = 4\n",
    "bias_std = 0.01\n",
    "noise_mean = 0.02\n",
    "noise_std = 0.004\n",
    "n_ensembles = 10\n",
    "hidden_size = 100  # Tune this as needed\n",
    "\n",
    "# Hyperparameters\n",
    "n = X_train.shape[0]\n",
    "x_dim = X_train.shape[1]\n",
    "alpha_dim = x_dim - num_models\n",
    "y_dim = y_train.shape[1]\n",
    "learning_rate = 0.00005\n",
    "n_epochs = 100  # Adjust as needed\n",
    "batch_size = 2000\n",
    "\n",
    "# Initialize standard deviations for weights and biases\n",
    "init_stddev_1_w = np.sqrt(3 / (x_dim - num_models))\n",
    "init_stddev_1_b = init_stddev_1_w\n",
    "init_stddev_2_w = 1.3 / np.sqrt(hidden_size)\n",
    "init_stddev_2_b = init_stddev_2_w\n",
    "init_stddev_3_w = (bias_std * 1.3) / np.sqrt(hidden_size)\n",
    "init_stddev_noise_w = (noise_std * 1.3) / np.sqrt(hidden_size)\n",
    "\n",
    "lambda_anchor = 1.0 / (np.array([init_stddev_1_w, init_stddev_1_b, init_stddev_2_w, init_stddev_2_b, init_stddev_3_w, init_stddev_noise_w])**2)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).squeeze()\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).squeeze()\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).squeeze()\n",
    "X_out  = torch.tensor(X_out, dtype=torch.float32).squeeze()\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).squeeze()\n",
    "y_out= torch.tensor(y_out, dtype=torch.float32).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4263, -0.0249,  1.4018, -0.8660,  0.5000,  0.7667],\n",
       "        [ 1.7143,  0.0000,  1.0301,  0.8660, -0.5000,  0.6667],\n",
       "        [ 1.3841,  0.3451,  1.4018,  0.8660,  0.5000,  0.0333],\n",
       "        ...,\n",
       "        [ 1.6519,  0.2913,  1.0893, -0.8660, -0.5000,  0.1333],\n",
       "        [ 1.6852, -0.1920,  1.0598, -0.8660, -0.5000,  0.1333],\n",
       "        [ 1.7102,  0.1196,  1.0301,  0.8660,  0.5000,  0.2333]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, num_models: num_models + alpha_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4844, -0.5857, -0.9773,  ..., -0.8660,  0.5000,  0.7667],\n",
       "        [ 0.6078,  0.2427, -0.8042,  ...,  0.8660, -0.5000,  0.6667],\n",
       "        [-0.3018, -0.8175, -0.5805,  ...,  0.8660,  0.5000,  0.0333],\n",
       "        ...,\n",
       "        [-0.9893,  0.7060,  0.1687,  ..., -0.8660, -0.5000,  0.1333],\n",
       "        [-0.5335, -0.2287,  0.0188,  ..., -0.8660, -0.5000,  0.1333],\n",
       "        [-0.7548, -0.5539,  0.1439,  ...,  0.8660,  0.5000,  0.2333]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, hidden_size, learning_rate, lambda_anchor, init_stddev_1_w, init_stddev_1_b, init_stddev_2_w, init_stddev_2_b, init_stddev_3_w, init_stddev_noise_w):\n",
    "        super(NN, self).__init__()\n",
    "        self.spacetime_dim = alpha_dim\n",
    "        self.modelpred_dim = num_models\n",
    "        self.y_dim = y_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_anchor = torch.tensor(lambda_anchor, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "\n",
    "        self.layer_1 = bnn.BayesLinear(prior_mu=0.0, prior_sigma=init_stddev_1_w, in_features=self.spacetime_dim, out_features=self.hidden_size, bias=True)\n",
    "        # ?? Bayesian batch normalization\n",
    "        self.layer_2 = bnn.BayesLinear(prior_mu=0.0, prior_sigma=init_stddev_2_w, in_features=self.hidden_size, out_features=self.modelpred_dim, bias=True)\n",
    "        self.modelbias = bnn.BayesLinear(prior_mu=0.0, prior_sigma=init_stddev_3_w, in_features=self.hidden_size, out_features=self.y_dim, bias=False)\n",
    "        self.noise_pred_layer = bnn.BayesLinear(prior_mu=0.0, prior_sigma=init_stddev_noise_w, in_features=self.hidden_size, out_features=self.y_dim, bias=False)\n",
    "\n",
    "\n",
    "        # Activation Function\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Save initial weights for anchoring\n",
    "        self.initial_weights = [param.data.clone() for param in self.parameters()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        modelpred = x[:, :self.modelpred_dim]\n",
    "        spacetime = x[:, self.modelpred_dim:]\n",
    "        #x = self.tanh(self.layer_1_bn(self.layer_1(spacetime)))\n",
    "        x = self.tanh(self.layer_1(spacetime))\n",
    "\n",
    "        modelpred = \n",
    "\n",
    "        model_coeff = torch.softmax(self.layer_2(x), dim=1)\n",
    "        modelbias = self.modelbias(x).squeeze(-1)\n",
    "        output = torch.sum(model_coeff * modelpred, dim=1) + modelbias\n",
    "        noise_pred = self.noise_pred_layer(x).squeeze(-1)\n",
    "        \n",
    "        return output, noise_pred, model_coeff, modelbias\n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_loss(self, x, y_target):\n",
    "        output, noise_pred, _, _ = self.forward(x)\n",
    "        \n",
    "        noise_sq = torch.square(noise_pred + noise_mean) + 1e-6\n",
    "        err_sq = torch.square(y_target.squeeze(-1) - output)\n",
    "        mse_ = torch.sum(err_sq)/ x.shape[0]\n",
    "        loss_ = (torch.sum(err_sq / noise_sq) + torch.sum(torch.log(noise_sq)))/ x.shape[0]\n",
    "\n",
    "        # Anchoring Loss\n",
    "        loss_anchor = 0\n",
    "        for lambda_val, param, initial_param in zip(self.lambda_anchor, self.parameters(), self.initial_weights):\n",
    "            loss_anchor += lambda_val * torch.sum((param - initial_param)**2) / x.shape[0]\n",
    "\n",
    "\n",
    "        \n",
    "        total_loss =  loss_ + loss_anchor\n",
    "\n",
    "        return mse_, loss_, loss_anchor, noise_sq, err_sq, total_loss\n",
    "\n",
    "\n",
    "    def train_step(self, x, y_target):\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        mse_loss, noise_loss, loss_anchor, noise_sq, err_sq, total_loss  = self.calculate_loss(x, y_target)\n",
    "        #total_loss = noise_loss + loss_anchor\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return total_loss.item(), mse_loss.item(), noise_loss.item(), loss_anchor.item() ,noise_sq.mean().item(), err_sq.mean().item()\n",
    "     \n",
    "    def get_noise_sq(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            _, noise_pred, _, _ = self.forward(x)\n",
    "            noise_sq = torch.square(noise_pred + noise_mean) + 1e-6\n",
    "        return noise_sq\n",
    "\n",
    "    def get_alphas(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            _, _, model_coeff, _ = self.forward(x)\n",
    "        return model_coeff\n",
    "\n",
    "    def get_betas(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            _, _, _, modelbias = self.forward(x)\n",
    "        return modelbias\n",
    "\n",
    "    def get_alpha_w(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.tanh(self.layer_1(x[:, self.modelpred_dim:]))\n",
    "            alpha_w = self.layer_2(x)\n",
    "        return alpha_w\n",
    "\n",
    "    def get_w1(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.tanh(self.layer_1(x[:, self.modelpred_dim:]))\n",
    "        return x\n",
    "\n",
    "def predict_ensemble(models, X):\n",
    "    with torch.no_grad():\n",
    "        all_preds = torch.stack([model(X)[0] for model in models])\n",
    "        all_noise_sq = torch.stack([model.get_noise_sq(X) for model in models])\n",
    "        all_alphas = torch.stack([model.get_alphas(X) for model in models])\n",
    "        all_betas = torch.stack([model.get_betas(X) for model in models])\n",
    "        all_alpha_w = torch.stack([model.get_alpha_w(X) for model in models])\n",
    "        all_w1 = torch.stack([model.get_w1(X) for model in models])\n",
    "\n",
    "    preds_mu = torch.mean(all_preds, dim=0)\n",
    "    preds_std = torch.std(all_preds, dim=0)\n",
    "\n",
    "    return all_preds, preds_mu, preds_std, all_noise_sq, all_alphas, all_betas, all_alpha_w, all_w1\n",
    "\n",
    "\n",
    "def train_ensemble(X_train, y_train, n_ensembles, n_epochs, batch_size):\n",
    "    models = []\n",
    "    num_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "    for ensemble_index in range(n_ensembles):\n",
    "        model = NN(x_dim, y_dim, hidden_size, learning_rate, lambda_anchor, init_stddev_1_w, init_stddev_1_b, init_stddev_2_w, init_stddev_2_b, init_stddev_3_w, init_stddev_noise_w)\n",
    "\n",
    "        losses = {\n",
    "            'total_losses': [],\n",
    "            'mse_losses': [],\n",
    "            'noise_losses': [],\n",
    "            'anchor_losses': [],\n",
    "        }\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle data before each epoch\n",
    "            permutation = torch.randperm(X_train.shape[0])\n",
    "            X_train_shuffled = X_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            epoch_losses = {\n",
    "                'total_loss': 0,\n",
    "                'mse_loss': 0,\n",
    "                'noise_loss': 0,\n",
    "                'anchor_loss': 0,\n",
    "                'count': 0\n",
    "            }\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch_x = X_train_shuffled[start_idx:end_idx]\n",
    "                batch_y = y_train_shuffled[start_idx:end_idx]\n",
    "\n",
    "                loss, mse_loss, noise_loss, anchor_loss, noise_sq, err_sq = model.train_step(batch_x, batch_y)\n",
    "\n",
    "                epoch_losses['total_loss'] += loss\n",
    "                epoch_losses['mse_loss'] += mse_loss\n",
    "                epoch_losses['noise_loss'] += noise_loss\n",
    "                epoch_losses['anchor_loss'] += anchor_loss\n",
    "                epoch_losses['count'] += 1\n",
    "\n",
    "            for key in epoch_losses:\n",
    "                if key != 'count':\n",
    "                    corrected_key = key + 'es' if key.endswith('loss') else key + 's'\n",
    "                    losses[corrected_key].append(epoch_losses[key] / epoch_losses['count'])\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Ensemble {ensemble_index + 1}, Epoch {epoch}, Total Loss: {losses['total_losses'][-1]}, MSE Loss: {np.round(np.sqrt(losses['mse_losses'][-1]),5)}, Noise Loss: {losses['noise_losses'][-1]}, Anchoring Loss: {losses['anchor_losses'][-1]}\")\n",
    "                print(f\"noise_sq {noise_sq}, err_sq {err_sq}\")\n",
    "\n",
    "        save_path = f\"{base_dir}model_{ensemble_index}_checkpoint.pth\"\n",
    "        save_model(model, model.optimizer, losses, save_path)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, losses, save_path=\"model_checkpoint.pth\"):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'losses': losses,  \n",
    "    }\n",
    "    torch.save(save_dict, save_path)\n",
    "    print(f\"Model, optimizer, and losses saved to {save_path}\")\n",
    "\n",
    "    \n",
    "def recube(in_tensor, lat_len, lon_len, time_len):\n",
    "    \"\"\"\n",
    "    Reshape a flat tensor to a 3D tensor based on dimensions of time, latitude, and longitude.\n",
    "    \"\"\"\n",
    "    output = in_tensor.reshape(time_len, lat_len, lon_len)\n",
    "    return output\n",
    "\n",
    "def report_on_percentiles_tensor(y, y_pred, y_std):\n",
    "    \"\"\"\n",
    "    Report the percentage of data points within 1, 2, and 3 standard deviations.\n",
    "    \"\"\"\n",
    "    y, y_pred, y_std = y.flatten(), y_pred.flatten(), y_std.flatten()\n",
    "    diffs = torch.abs(y_pred - y)\n",
    "    within_1_std = torch.sum(diffs <= y_std * 1).item() / y.shape[0]\n",
    "    within_2_std = torch.sum(diffs <= y_std * 2).item() / y.shape[0]\n",
    "    within_3_std = torch.sum(diffs <= y_std * 3).item() / y.shape[0]\n",
    "\n",
    "    print(f'Using {y.shape[0]} data points')\n",
    "    print(f'{within_1_std * 100:.2f}% within 1 std')\n",
    "    print(f'{within_2_std * 100:.2f}% within 2 std')\n",
    "    print(f'{within_3_std * 100:.2f}% within 3 std')\n",
    "\n",
    "def report_on_percentiles(y, y_pred, y_std):\n",
    "\n",
    "    n = len(y.ravel())\n",
    "\n",
    "    n1 = np.sum(np.abs(y_pred.ravel() - y.ravel()) <= y_std.ravel() * 1)\n",
    "    n2 = np.sum(np.abs(y_pred.ravel() - y.ravel()) <= y_std.ravel() * 2)\n",
    "    n3 = np.sum(np.abs(y_pred.ravel() - y.ravel()) <= y_std.ravel() * 3)\n",
    "    print('Using {} data points'.format(n))\n",
    "    print('{} within 1 std'.format(100 * n1 / n))\n",
    "    print('{} within 2 std'.format(100 * n2 / n))\n",
    "    print('{} within 3 std'.format(100 * n3 / n))\n",
    "\n",
    "    return\n",
    "\n",
    "def get_betas(models, X):\n",
    "    with torch.no_grad():  \n",
    "        betas = torch.stack([model(X)[3] for model in models]) \n",
    "    return betas\n",
    "\n",
    "\n",
    "def get_alphas(models, X):\n",
    "    with torch.no_grad():\n",
    "        coeffs = [model(X)[2] for model in models]  \n",
    "    coeffs_tensor = torch.stack(coeffs)\n",
    "    return coeffs_tensor\n",
    "\n",
    "def calculate_rmse(true, pred_mu):\n",
    "    rmse = torch.sqrt(torch.mean(torch.square(pred_mu - true)))\n",
    "    return rmse\n",
    "\n",
    "def calculate_nll(true, pred_mu, pred_std):\n",
    "    nll = 0.5 * (((pred_mu - true) ** 2) / (pred_std ** 2)) + torch.log(pred_std ** 2) + torch.log(2 * torch.tensor(np.pi))\n",
    "    return torch.mean(nll)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
